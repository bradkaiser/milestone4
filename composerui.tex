\documentclass{article}

\usepackage{times}
\usepackage{uist07}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{eufrak}

\begin{document}

% --- Copyright notice ---
\CopyrightYear{2013}

% Uncomment the following line to hide the copyright notice
 \toappear{}
 %------------------------

\title{Transcribing a Sung or Whistled Melody in the Browser: Composer}

%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

\author{
\parbox[t]{7cm}{\centering
	     {\em Brad Kaiser}\\
	     Georgia Institute of Technology\\
             225 North Ave NW\\
	     Atlanta, Ga 30332\\
	     gtg302b@mail.gatech.edu}
\parbox[t]{7cm}{\centering
	     {\em Vinai Suresh}\\
	     Georgia Institute of Technology\\
             251 10th St. NW\\
	     Atlanta, Ga 30318\\
	     vinaisuresh9@mail.gatech.edu}	
}

\maketitle

\section{ABSTRACT}
This paper presents a system for transcribing a whistled or sung melody directly into
musical notation for the purpose of musical composition. We are extending 
previous work in this area in several ways with a focus on making composition 
extremely easy for the laymen. First, we are implementing solely in the browser, which
will make our application much more accessible than if it was an compiled program that had to 
be downloaded and installed. Second, we will combine signal processing techniques to allow for the more complex case of sung input. 
Finally, we will include a mechanism for inputting a rhythm. 

\classification{
H.5.1 [Information interfaces and Presentation]:
Multimedia Information Systems - Audio input output; H.5.3
[Information Interfaces and Presentation]: Group and Organization Interfaces - Web-based interaction}

\terms{Human Factors}

\keywords{Music Audio Web}

\tolerance=400 
  % makes some lines with lots of white space, but 	
  % tends to prevent words from sticking out in the margin

\section{INTRODUCTION}
Computers have a long history of lowering the barrier of entry to different modes of creative expression. It is now easier than ever to edit our own video, 
manipulate our own photos, and publish our writing. We'd like to extend this ease to musical composition. To write out a musical composition requires
formal musical skills that most people do not have. However most people do have some innate musical talent, and many could whistle or sing a tune easily. If we could harness these informal skills, we would sidestep a huge
barrier to entry. There has been successful work in this area, see \cite{2011Toh,2010Shen}. We aim to extend this work by first making our application effortlessly accessible. 
This entail putting it on the web and implementing in the browser. Second we  will accept voiced input and not just whistling. Third, these implementations are only concerned with melody, we aim to also have the means of inputting a rhythm.

\section{IMPLEMENTATION}
This application was developed for the browser using modern web technologies such as HTML5, CSS3, SVG, and the JavaScript Web Audio API.
It was our aim to use only web standard technologies and not any plugins.

We chose to base our implementation around Chrome instead of the other major web browsers because it is very popular, fast, and supported the Web Audio API.
Firefox only supported a deprecated version of the API until very recently, and Internet Explorer does not suport the API.
Additionally, Chrome works best with the MIDI library we used, and supports high quality ogg format soundfonts, for use with MIDI.
For the scope of this project we did not want to spend effort ensuring cross-browser compatibility for our app.
In the future we would could make our app work with more browsers as the Web Audio API becomes more widely supported.
According to the W3C statistics of this year, Chrome is the most popular browser, so our scope covers a large majority of internet users.

\subsection{Architecture}
This application needs to solve two major problems. 
How to translate audio input into musical notation, and how to display musical notation output.
To make development easier we decoupled these two problems.
We wrote a note detecting engine that sets up the microphone connection using the web audio api and processes the raw audio input.
When it picks out a note from the microphone it publishes a custom javascript event.
We also wrote a note layout engine that listens for these event, and keeps track of a musical staff and can add notes and lay them out correctly.
When it intercepts a note event, it adds the note to DOM.

\section{INPUT}
Our application takes a variety of inputs easily produced by most people such as whistling, singing, and clapping.
Initially we were concerned that we would require the user to specify what kind of input they were providing, but this turned out to unnecessary.
We can run our pitch detection and beat detection concurrently without any issue. Our pitch detection library also works on both whistling and singing. 

\subsection{Semantic Snapping and Constraints}
While this project is predicated on the idea that the average person has some innate musical talent, it is clear that most people can't carry a tune or keep a beat very well.
To account for this we snap user input to the nearest semitone and note duration.
We do not try to detect anything shorter than a thirty-second note.
We assume a standard A440 tuning which sets middle A at 440 Hz, and we assume a tempo of 120 bpm (beats per minute) in 4 4 time, which is a common moderate tempo.
These assumptions are hardcoded, though it would be easier in the future to make them adjustable. 

\subsection{Pitch Detection}
We tried several different algorithms for pitch detection before settling on an open source solution based on wavelet analysis \cite{2010Schmitt}.
In the initial conception for this application, we would only accept whistled input, and use straightforward fourier analysis to detect the pitch.
In practice this worked ok.
A whistle is close enough to a pure tone that its spectrum has a nice clearly defined peak.
There were two big problems with this approach.
The first is that it does not work at all for sung input, the spectrum of the human voice is complicated, and does not have a clear dominant frequency.
The second is that at our sampling rate, 44100Hz, we only just have enough time and frequency resolution to separate semitones within the span of a thirty-second note.
At our 120 bpm a thirty-second note is 62.5 ms long, so we can take at most 2756 samples at a time to analyze.
Since the discrete fourier transform demands samples in a power of two, we take 2048 samples.
With that many samles, we only have a frequency resolution of 21Hz.
This is enough to differentiate semitones at the relatively high frequencies where whistling occurs, but since the musical scale is logarithmic, it is no longer enough once you get around middle C.

Since we wanted to pitch detect sung as well as whistled input, we tried another pitch detection technique, cepstral analysis pioneered by Noll in 1967 \cite{1967Noll}.
As Noll explains it, the output of human speech \(f(t)\) is the product of a source signal \(s(t)\) convolved with the response of the vocal tract \(h(t)\). 

\[f(t) = s(t) \ast h(t)\] 

This means the power spectrum of the output signal will be the product of the power spectra of the source and vocal tract.

\[|F(\omega)|^2 = |S(\omega)|^2 \cdot |H(\omega)|^2 \]

We are only interested in the source signal, so it is necessary to make it easily seperable and identifiable from the tract signal.
Noll suggests the logarithm to make the two signals additive.

\[\log(|F(\omega)|^2) = \log(|S(\omega)|^2 |H(\omega)|^2) \]
\[\log(|F(\omega)|^2) = \log(|S(\omega)|^2) + \log(|H(\omega)|^2) \]

Finally, since the source spectra varies quickly and the tract spectra varies slowly, we can apply the fourier transform again, and completely separate the two signals by looking at the "frequency" of their spectra.

\[\mathfrak{F}[\log(|F(\omega)|^2)] = \mathfrak{F}[\log(|S(\omega)|^2)] + \mathfrak{F}[\log(|H(\omega)|^2)] \]

In practice, we were never able to get a clear separation between the source and tract signals, and could not use this method for pitch detection.

Fortunately, we found an open source pitch tracking library, dywapitchtrack \cite{2010Schmitt}.
The library was written in C, so we had to compile it into javascript with emscripten.
The library takes an array of raw audio data and returns a frequency and a confidence value. 
It also detects whether there is currently input or not. If the input is silent, it returns a null pitch value.
It was able to pitch detect a human voice very well, as well as whistling. 

\subsection{Note Separation}
Though the dywapitchtrack library neatly solved our pitch detection problem, we still do not have notes, just a stream of frequency values.
We need to determine a note's duration and pitch.
To do this the application has a buffer where we can accumulate frequency results and examine them with context.
We accumulate ten results, then break them into groups by null results.
We found that the dywapitchtrack algorithm frequently returned spurious low frequency results during periods of silence, so if any group of results would be shorter than a thirty-second note, we drop it.
Any remaining groups are assigned a note type based on their duration and passed to the layout engine. 
The note type is the largest that will fit in the duration of the result group.
The pitch assigned to the note is the median of the frequency results included. 
If there are results at the end of the buffer, we leave them to be examined in the next cycle. This way, we can avoid splitting up long notes.

\begin{figure}[h]
    \caption{An example of the spectrum and cepstrum of the human voice. \cite{ee649}}
    \includegraphics[width=0.5\textwidth]{cepstrum.pdf}
\end{figure}

\subsection{Beat Detection}
Beat detection was considerably easier than pitch detection. We wanted to be able to detect when a user clapped or snapped, and treat that like a percussion instrument.
We passed the raw audio input through a high pass filter to remove all of the melodic frequency components. 
This only left high frequency components from staccatto noises. 
Then we just registered a percussion event whenever the intensity of the resulting signal passed a certain threshold. 
We determined the cutoff frequency and power thresholed empirically.
The only way to determine how long a drum beat should be is to wait for the next drum beat, so for beat detection we set up a two element buffer. 
When a beat is detected, it sits in the buffer until the next beat is detected.
At that point we can determine the duration of the first beat, so we publish it and remove it from the buffer.
Then the second beat has to wait. 

\section{Output}
Our application has several modes of output. As the user is working, they are be able to listen to their composition played to them through MIDI. This gives a preview of the final result and makes it possible to edit the composition. When the user is finished with the composition, they are able to print the sheet music using standard browser printing. We had originally wanted to allow for output in MusicXML, which as a widely used music format for digital applications. This format would make this music far more extensible and compatible with other applications that use this standard. However we decided that it would be used in future iterations of this project.   For simplicity, only common time, 4 quarter notes to a measure,  were used for this musical notation, as well as not confusing sharps and flats, but resorting to all flats for any half tone. \cite{2004Baird}.

The Midi output of the application is done using the Midi.js library \cite{midi.js}. This library provides an easy interface for deciding which audio API to use, depending on the browser. The Web Audio API is a new standard that has recently been adopted by Chrome. We decided to focus on chrome for this project, as it had the best sound quality. When playing music back, the user has the option of playing all the music, or just parts of it, and is able to stop the playback at any time. When selecting a note, that note is also played briefly so the user knows the tone. The process of selecting notes and parts of music to playback will be described more in the section below.

\section{User Interface}
Our main goal for the Composer interface was to keep it as simple as possible. We wanted it to be
easy to understand without much text or unnecessary UI. However, we decided to include a small tutorial that explains the more detailed usages of the application. 

The user interface is based primarily on the open source library Vexflow \cite{vexflow}. This library is currently in an alpha stage, and thus required some modification for our purposes. Vexflow provided the necessary tools for static layout of staves and notes, however it did not provide much in the way of animation or dynamic rendering in general. For ease of use, especially for animations, we used the SVG tools instead of Canvas, so that we could treat each note object as an element in the DOM. We created classes for each note, so that all svg elements related to that note would be linked together. This allowed for us to provide the animations detailed below. We also had to create a way to dynamically render the UI by redrawing all elements when new notes and bars were added. This provided an easy way for us to draw all notes with a few simple method calls. 

The notes are animated in with a bounce animation.As we described in our earlier proposal, animations were a key part of the user experience. We wanted the users to feel immersed in the program without unnecessary background visual "noise".  Animation has been show to increase the appeal of a user interface \cite{2008Dyer}. Check \cite{2012Joshi} for the different types of animations and transitions available with CSS3.  Once the notes are on the page, 

On the screen there will be a couple buttons that allow for the user to change their recording mode, as well as if they are currently recording. The main button will be the record/stop button which will tell the program to start listening for musical input and when it should stop. Another button will deal with whether the user is in beat input mode or melody input mode. Since users  will only be able to input a monochromatic melody, there will be no structure for inputting chords. Therefore the melody input mode will only allow for single note types, whether whistled or hummed. The beat input mode will allow for the users to make non-pitch input, such as clapping or banging on other objects, such as a desk. These will be translated to a drum-type sound which will be used for the actual sound output. 


\begin{figure}[t]
    \caption{Simple layout of web application.}
    \includegraphics[width=0.5\textwidth, height=0.35\textheight]{blank_page.png}
\end{figure}


\begin{figure}[h]
    \caption{Example of Clefs. Bass and Treble clefs used for melody, bottom bar for Percussion}
    \includegraphics[width=0.5\textwidth]{clefs.png}
\end{figure}


\subsection{Affordances}


Affordance will be plenty, in that with the simple interface, it will be easy for users to know what they should do. The user will be presented with a blank page of sheet music, as well as two buttons, one for recording and one for changing the input type. The users will know that the record button will be used for actually recording the music, and when recording, this button will change to a stop button to show that the user will press it to stop recording.

The user will be able to know that the longer they hum a note, the longer it will appear on the page. This will happen relatively easily, as the user will delve into using the application and pick up very quickly that it responds to the length of time hummed or whistled.

Notes will also have a depth-like quality to them, indicating that they can be picked up and moved on a flat background.

\subsection{Feedback}
Feedback will include a play button that will allow the user to playback the music that is currently recorded. This will play back in a MIDI format. Once the user has clicked on a certain section of the music, they can playback from that section to the end of recorded music. 

When a user clicks a note, the note will be highlighted, indicating that the note is the current focus of the program. From there the user will be able to change the length of the note, as well as where it is located within the sheet music. If the user clicks on the playback button mentioned above, the music will playback from the highlighted note to the end of the song.

Hovering over a note will change the cursor and show the user that they will be able to interact with individual notes. This is a key feedback mechanism, as it will show the user that they have a more direct control of the music, instead of just microphone input.


\subsection{Mechanics}
The quality of the mechanics of the user interface of our app depend largely on the quality of our interpretation of the user input. 
If we are successful in accurately transcribing the user's input, then the mechanics of our app should be excellent, because we will
be capitalizing on common skills the user practices every day. 

We can improve the mechanics of the app further by making use of semantic snapping to gently correct the user's input. Though we are using well 
practiced skills, the user will probably be off key, and their timing will probably be slightly off.  We will correct this by snapping the input to the nearest quarter 
tone and standard note duration. This way our interpretation of the user's performance is better than perfect. 

\section{FUTURE WORK}
There are several areas where substantial improvement could be made to this app.
First there is a lot more work that we could do on the note detecting component. 
It currently requires you to sing very clearly and make your notes very clearly separated to pick them up well. 
It does not support rests, or keep the percurssion and melody in sync.
Finally, notes tend to get split when they occur at the end of a bar.
Fortunately we still have lots of tools at our disposal to remedy these shortcomings. 
There is a wealth of audio processing techniques that could still be incorporated into the app.

We also did not implement the MusicXML and MIDI output feature we described in our proposal. 
These features were not vital to our main focus of music composition, and would not be interesting in their implementation.
These could be easily implemented later if desired.


\nocite{*}


\bibliographystyle{plain}
\bibliography{UIProject}

\end{document}
