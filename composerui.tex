\documentclass{article}

\usepackage{times}
\usepackage{uist07}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{eufrak}

\begin{document}

% --- Copyright notice ---
\CopyrightYear{2013}

% Uncomment the following line to hide the copyright notice
 \toappear{}
 %------------------------

\title{Transcribing a Sung or Whistled Melody in the Browser: Composer}

%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

\author{
\parbox[t]{7cm}{\centering
	     {\em Brad Kaiser}\\
	     Georgia Institute of Technology\\
             225 North Ave NW\\
	     Atlanta, Ga 30332\\
	     gtg302b@mail.gatech.edu}
\parbox[t]{7cm}{\centering
	     {\em Vinai Suresh}\\
	     Georgia Institute of Technology\\
             251 10th St. NW\\
	     Atlanta, Ga 30318\\
	     vinaisuresh9@mail.gatech.edu}	
}

\maketitle

\section{ABSTRACT}
This paper presents a system for transcribing a whistled or sung melody directly into
musical notation for the purpose of musical composition. We are extending 
previous work in this area in several ways with a focus on making composition 
extremely easy for the laymen. First, we are implementing solely in the browser, which
will make our application much more accessible than if it was an compiled program that had to 
be downloaded and installed. Second, we will combine signal processing techniques to allow for the more complex case of sung input. 
Finally, we will include a mechanism for inputting a rhythm. 

\classification{
H.5.1 [Information interfaces and Presentation]:
Multimedia Information Systems - Audio input output; H.5.3
[Information Interfaces and Presentation]: Group and Organization Interfaces - Web-based interaction}

\terms{Human Factors}

\keywords{Music Audio Web}

\tolerance=400 
  % makes some lines with lots of white space, but 	
  % tends to prevent words from sticking out in the margin

\section{INTRODUCTION}
Computers have a long history of lowering the barrier of entry to different modes of creative expression. It is now easier than ever to edit our own video, 
manipulate our own photos, and publish our writing. We'd like to extend this ease to musical composition. To write out a musical composition requires
formal musical skills that most people do not have. However most people do have some innate musical talent, and many could whistle or sing a tune easily. If we could harness these informal skills, we would sidestep a huge
barrier to entry. There has been successful work in this area, see \cite{2011Toh,2010Shen}. We aim to extend this work by first making our application effortlessly accessible. 
This entail putting it on the web and implementing in the browser. Second we  will accept voiced input and not just whistling. Third, these implementations are only concerned with melody, we aim to also have the means of inputting a rhythm.

\section{INPUT}
Our application takes a variety of inputs easily produced by most people such as whistling, singing, and clapping.
Initially we were concerned that we would require the user to specify what kind of input they were providing, but this turned out to unnecessary.
We can run our pitch detection and beat detection concurrently without any issue. Our pitch detection library also works on both whistling and singing. 

\subsection{Semantic Snapping and Constraints}
While this project is predicated on the idea that the average person has some innate musical talent, it is clear that most people can't carry a tune or keep a beat very well.
To account for this we snap user input to the nearest semitone and note duration.
We do not try to detect anything shorter than a thirty-second note.
We assume a standard A440 tuning which sets middle A at 440 Hz, and we assume a tempo of 120 bpm (beats per minute) in 4 4 time, which is a common moderate tempo.
These assumptions are hardcoded, though it would be easier in the future to make them adjustable. 

\subsection{Pitch Detection}
We tried several different algorithms for pitch detection before settling on an open source solution based on wavelet analysis \cite{2010Schmitt}.
In the initial conception for this application, we would only accept whistled input, and use straightforward fourier analysis to detect the pitch.
In practice this worked ok.
A whistle is close enough to a pure tone that its spectrum has a nice clearly defined peak.
There were two big problems with this approach.
The first is that it does not work at all for sung input, the spectrum of the human voice is complicated, and does not have a clear dominant frequency.
The second is that at our sampling rate, 44100Hz, we only just have enough time and frequency resolution to separate semitones within the span of a thirty-second note.
At our 120 bpm a thirty-second note is 62.5 ms long, so we can take at most 2756 samples at a time to analyze.
Since the discrete fourier transform demands samples in a power of two, we take 2048 samples.
With that many samles, we only have a frequency resolution of 21Hz.
This is enough to differentiate semitones at the relatively high frequencies where whistling occurs, but since the musical scale is logarithmic, it is no longer enough once you get around middle C.

Since we wanted to pitch detect sung as well as whistled input, we tried another pitch detection technique, cepstral analysis pioneered by Noll in 1967 \cite{1967Noll}.
As Noll explains it, the output of human speech \(f(t)\) is the product of a source signal \(s(t)\) convolved with the response of the vocal tract \(h(t)\). 

\[f(t) = s(t) \ast h(t)\] 

This means the power spectrum of the output signal will be the product of the power spectra of the source and vocal tract.

\[|F(\omega)|^2 = |S(\omega)|^2 \cdot |H(\omega)|^2 \]

We are only interested in the source signal, so it is necessary to make it easily seperable and identifiable from the tract signal.
Noll suggests the logarithm to make the two signals additive.

\[\log(|F(\omega)|^2) = \log(|S(\omega)|^2 |H(\omega)|^2) \]
\[\log(|F(\omega)|^2) = \log(|S(\omega)|^2) + \log(|H(\omega)|^2) \]

Finally, since the source spectra varies quickly and the tract spectra varies slowly, we can apply the fourier transform again, and completely separate the two signals by looking at the "frequency" of their spectra.

\[\mathfrak{F}[\log(|F(\omega)|^2)] = \mathfrak{F}[\log(|S(\omega)|^2)] + \mathfrak{F}[\log(|H(\omega)|^2)] \]

In practice, we were never able to get a clear separation between the source and tract signals, and could not use this method for pitch detection.

Fortunately, we found an open source pitch tracking library, dywapitchtrack \cite{2010Schmitt}.
The library was written in C, so we had to compile it into javascript with emscripten.
The library takes an array of raw audio data and returns a frequency and a confidence value. 
It also detects whether there is currently input or not. If the input is silent, it returns a null pitch value.
It was able to pitch detect a human voice very well, as well as whistling. 

\subsection{Note Separation}
Though the dywapitchtrack library neatly solved our pitch detection problem, we still do not have notes, just a stream of frequency values.
We need to determine a note's duration and pitch.
To do this the application has a buffer where we can accumulate frequency results and examine them with context.
We accumulate ten results, then break them into groups by null results.
We found that the dywapitchtrack algorithm frequently returned spurious low frequency results during periods of silence, so if any group of results would be shorter than a thirty-second note, we drop it.
Any remaining groups are assigned a note type based on their duration and passed to the layout engine. 
The note type is the largest that will fit in the duration of the result group.
The pitch assigned to the note is the median of the frequency results included. 
If there are results at the end of the buffer, we leave them to be examined in the next cycle. This way, we can avoid splitting up long notes.

\begin{figure}[h]
    \caption{An example of the spectrum and cepstrum of the human voice. \cite{ee649}}
    \includegraphics[width=0.5\textwidth]{cepstrum.pdf}
\end{figure}

\subsection{Beat Detection}
Beat detection was considerably easier than pitch detection. We wanted to be able to detect when a user clapped or snapped, and treat that like a percussion instrument.
We passed the raw audio input through a high pass filter to remove all of the melodic frequency components. 
This only left high frequency components from staccatto noises. 
Then we just registered a percussion event whenever the intensity of the resulting signal passed a certain threshold. 
We determined the cutoff frequency and power thresholed empirically.
The only way to determine how long a drum beat should be is to wait for the next drum beat, so for beat detection we set up a two element buffer. 
When a beat is detected, it sits in the buffer until the next beat is detected.
At that point we can determine the duration of the first beat, so we publish it and remove it from the buffer.
Then the second beat has to wait. 

\section{Output}
Our application has several modes of output. As the user is working, they are be able to listen to their composition played to them through MIDI. This gives a preview of the final result and makes it possible to edit the composition. When the user is finished with the composition, they are able to print the sheet music using standard browser printing. We had originally wanted to allow for output in MusicXML, which as a widely used music format for digital applications. This format would make this music far more extensible and compatible with other applications that use this standard. However we decided that it would be used in future iterations of this project.   For simplicity, only common time, 4 quarter notes to a measure,  were used for this musical notation, as well as not confusing sharps and flats, but resorting to all flats for any half tone. \cite{2004Baird}.

The Midi output of the application is done using the Midi.js library \cite{midi.js}. This library provides an easy interface for the 

\section{User Interface}
Our main goal for the Composer interface was to keep it as simple as possible. We wanted it to be
easy to understand without much text or unnecessary UI. However, we decided to include a small tutorial that explains the more detailed usages of the application. 

The user interface is based primarily on the open source library Vexflow \cite{vexflow}. This library 

The notes are animated in with a bounce animation.As we described in our earlier proposal, animations were a key part of the user experience. We wanted the users to feel immersed in the program without unnecessary background visual "noise".  Animation has been show to increase the appeal of a user interface \cite{2008Dyer}. Check \cite{2012Joshi} for the different types of animations and transitions available with CSS3.  Once the notes are on the page 

On the screen there will be a couple buttons that allow for the user to change their recording mode, as well as if they are currently recording. The main button will be the record/stop button which will tell the program to start listening for musical input and when it should stop. Another button will deal with whether the user is in beat input mode or melody input mode. Since users  will only be able to input a monochromatic melody, there will be no structure for inputting chords. Therefore the melody input mode will only allow for single note types, whether whistled or hummed. The beat input mode will allow for the users to make non-pitch input, such as clapping or banging on other objects, such as a desk. These will be translated to a drum-type sound which will be used for the actual sound output. 


\begin{figure}[t]
    \caption{Example of simple layout of web application.}
    \includegraphics[width=0.5\textwidth]{blank_page.png}
\end{figure}


\begin{figure}[h]
    \caption{Example of Clefs. Top bar will be used for melody, and bottom bar will be used for percussion.}
    \includegraphics[width=0.5\textwidth]{clefs_with_drums.png}
\end{figure}




\section{Implementation}
Our app is going to be implemented in the browser using HTML5 and javascript without any browser plugins like Flash or Silverlight. 
This has only recently become possible with the inclusion of the web audio api in HTML5. We will also be using open source javascript 
libraries whenever available to avoid duplicating any programming effort. MIDI.js will provide the midi playback of the user's composition. 
To help analyze our audio input we will expand upon the functionality in Dsp.js. To logically represent the user's musical input, we will take 
advantage of MusicJSON, which is a JSON version of MusicXML an open format for representing musical notation. Finally, we will take some inspiration from VexFlow, a 
javascript library for representing notes and staves using the HTML canvas element.

\subsection{Affordance}

Affordance will be plenty, in that with the simple interface, it will be easy for users to know what they should do. The user will be presented with a blank page of sheet music, as well as two buttons, one for recording and one for changing the input type. The users will know that the record button will be used for actually recording the music, and when recording, this button will change to a stop button to show that the user will press it to stop recording.

The user will be able to know that the longer they hum a note, the longer it will appear on the page. This will happen relatively easily, as the user will delve into using the application and pick up very quickly that it responds to the length of time hummed or whistled.

Notes will also have a depth-like quality to them, indicating that they can be picked up and moved on a flat background.

\subsection{Feedback}
Feedback will include a play button that will allow the user to playback the music that is currently recorded. This will play back in a MIDI format. Once the user has clicked on a certain section of the music, they can playback from that section to the end of recorded music. 

When a user clicks a note, the note will be highlighted, indicating that the note is the current focus of the program. From there the user will be able to change the length of the note, as well as where it is located within the sheet music. If the user clicks on the playback button mentioned above, the music will playback from the highlighted note to the end of the song.

Hovering over a note will change the cursor and show the user that they will be able to interact with individual notes. This is a key feedback mechanism, as it will show the user that they have a more direct control of the music, instead of just microphone input.


\subsection{Mechanics}
The quality of the mechanics of the user interface of our app depend largely on the quality of our interpretation of the user input. 
If we are successful in accurately transcribing the user's input, then the mechanics of our app should be excellent, because we will
be capitalizing on common skills the user practices every day. 

We can improve the mechanics of the app further by making use of semantic snapping to gently correct the user's input. Though we are using well 
practiced skills, the user will probably be off key, and their timing will probably be slightly off.  We will correct this by snapping the input to the nearest quarter 
tone and standard note duration. This way our interpretation of the user's performance is better than perfect. 

\section{RESPONSE TO PRELIMINARY PROPOSAL}
In response to the feedback from our preliminary proposal we have made several changes to our project. First we are accepting voiced input which will require analysis beyond just a FFT. As such we cannot rely completely on the web audio api. Secondly we are accepting a second mode of input for the beat. Not only will this be a fun feature, but it is also a new feature that we have not seen in any other music transcription applications in the literature.

\nocite{*}


\bibliographystyle{plain}
\bibliography{UIProject}

\end{document}
