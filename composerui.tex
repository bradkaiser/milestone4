\documentclass{article}

\usepackage{times}
\usepackage{uist07}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{eufrak}

\begin{document}

% --- Copyright notice ---
\CopyrightYear{2013}

% Uncomment the following line to hide the copyright notice
 \toappear{}
 %------------------------

\title{Transcribing a Sung or Whistled Melody in the Browser: Composer}

%%
%% Note on formatting authors at different institutions, as shown below:
%% Change width arg (currently 7cm) to parbox commands as needed to
%% accommodate widest lines, taking care not to overflow the 17.8cm line width.
%% Add or delete parboxes for additional authors at different institutions. 
%% If additional authors won't fit in one row, you can add a "\\"  at the
%% end of a parbox's closing "}" to have the next parbox start a new row.
%% Be sure NOT to put any blank lines between parbox commands!
%%

\author{
\parbox[t]{7cm}{\centering
	     {\em Brad Kaiser}\\
	     Georgia Institute of Technology\\
             225 North Ave NW\\
	     Atlanta, Ga 30332\\
	     gtg302b@mail.gatech.edu}
\parbox[t]{7cm}{\centering
	     {\em Vinai Suresh}\\
	     Georgia Institute of Technology\\
             251 10th St. NW\\
	     Atlanta, Ga 30318\\
	     vinaisuresh9@mail.gatech.edu}	
}

\maketitle

\section{ABSTRACT}
This paper presents a system for transcribing a whistled or sung melody directly into
musical notation for the purpose of musical composition. We are extending 
previous work in this area in several ways with a focus on making composition 
extremely easy for the laymen. First, we are implementing solely in the browser, which makes our application much more accessible than if it was an compiled program that had to 
be downloaded and installed. Second, we combine signal processing techniques to allow for the more complex case of sung input. 
Finally, we include a mechanism for inputting a rhythm. 

\classification{
H.5.1 [Information interfaces and Presentation]:
Multimedia Information Systems - Audio input output; H.5.3
[Information Interfaces and Presentation]: Group and Organization Interfaces - Web-based interaction}

\terms{Human Factors}

\keywords{Music Audio Web}

\tolerance=400 
  % makes some lines with lots of white space, but 	
  % tends to prevent words from sticking out in the margin

\section{INTRODUCTION}
Computers have a long history of lowering the barrier of entry to different modes of creative expression. It is now easier than ever to edit our own video, 
manipulate our own photos, and publish our writing. We'd like to extend this ease to musical composition. To write out a musical composition requires
formal musical skills that most people do not have. However most people do have some innate musical talent, and many could whistle or sing a tune easily. If we could harness these informal skills, we would sidestep a huge
barrier to entry. There has been successful work in this area, see \cite{2011Toh,2010Shen}. We aim to extend this work by first making our application effortlessly accessible. 
This entail putting it on the web and implementing in the browser. Second we  accept voiced input and not just whistling. Third, as these previous implementations are only concerned with melody, we also have the means of inputting a rhythm.

\section{IMPLEMENTATION}
This application was developed for the browser using modern web technologies such as HTML5, CSS3, SVG, and the JavaScript Web Audio API.
It was our aim to use only web standard technologies and not any plugins.

We chose to base our implementation around Chrome instead of the other major web browsers because it is very popular, fast, and supported the Web Audio API.
Firefox only supported a deprecated version of the API until very recently, and Internet Explorer does not suport the API.
Additionally, Chrome works best with the MIDI library we used, and supports high quality ogg format soundfonts, for use with MIDI.
For the scope of this project we did not want to spend effort ensuring cross-browser compatibility for our app.
In the future we would could make our app work with more browsers as the Web Audio API becomes more widely supported.
According to the W3C statistics of this year, Chrome is the most popular browser, so our scope covers a large majority of internet users.

\subsection{Architecture}
This application needs to solve two major problems. 
How to translate audio input into musical notation, and how to display musical notation output.
To make development easier we decoupled these two problems.
We wrote a note detecting engine that sets up the microphone connection using the web audio api and processes the raw audio input.
When it picks out a note from the microphone it publishes a custom javascript event.
We also wrote a note layout engine that listens for these event, and keeps track of a musical staff and can add notes and lay them out correctly.
When it intercepts a note event, it adds the note to DOM.

\section{INPUT}
Our application takes a variety of inputs easily produced by most people such as whistling, singing, and clapping.
Initially we were concerned that we would require the user to specify what kind of input they were providing, but this turned out to unnecessary.
We can run our pitch detection and beat detection concurrently without any issue. Our pitch detection library also works on both whistling and singing. 

\subsection{Semantic Snapping and Constraints}
While this project is predicated on the idea that the average person has some innate musical talent, it is clear that most people can't carry a tune or keep a beat very well.
To account for this we snap user input to the nearest semitone and note duration.
We do not try to detect anything shorter than a thirty-second note.
We assume a standard A440 tuning which sets middle A at 440 Hz, and we assume a tempo of 120 bpm (beats per minute) in 4 4 time, which is a common moderate tempo.
These assumptions are hardcoded, though it would be easier in the future to make them adjustable. 

\subsection{Pitch Detection}
We tried several different algorithms for pitch detection before settling on an open source solution based on wavelet analysis \cite{2010Schmitt}.
In the initial conception for this application, we would only accept whistled input, and use straightforward fourier analysis to detect the pitch.
In practice this worked ok.
A whistle is close enough to a pure tone that its spectrum has a nice clearly defined peak.
There were two big problems with this approach.
The first is that it does not work at all for sung input, the spectrum of the human voice is complicated, and does not have a clear dominant frequency.
The second is that at our sampling rate, 44100Hz, we only just have enough time and frequency resolution to separate semitones within the span of a thirty-second note.
At our 120 bpm a thirty-second note is 62.5 ms long, so we can take at most 2756 samples at a time to analyze.
Since the discrete fourier transform demands samples in a power of two, we take 2048 samples.
With that many samles, we only have a frequency resolution of 21Hz.
This is enough to differentiate semitones at the relatively high frequencies where whistling occurs, but since the musical scale is logarithmic, it is no longer enough once you get around middle C.

Since we wanted to pitch detect sung as well as whistled input, we tried another pitch detection technique, cepstral analysis pioneered by Noll in 1967 \cite{1967Noll}.
As Noll explains it, the output of human speech \(f(t)\) is the product of a source signal \(s(t)\) convolved with the response of the vocal tract \(h(t)\). 

\[f(t) = s(t) \ast h(t)\] 

This means the power spectrum of the output signal will be the product of the power spectra of the source and vocal tract.

\[|F(\omega)|^2 = |S(\omega)|^2 \cdot |H(\omega)|^2 \]

We are only interested in the source signal, so it is necessary to make it easily seperable and identifiable from the tract signal.
Noll suggests the logarithm to make the two signals additive.

\[\log(|F(\omega)|^2) = \log(|S(\omega)|^2 |H(\omega)|^2) \]
\[\log(|F(\omega)|^2) = \log(|S(\omega)|^2) + \log(|H(\omega)|^2) \]

Finally, since the source spectra varies quickly and the tract spectra varies slowly, we can apply the fourier transform again, and completely separate the two signals by looking at the "frequency" of their spectra.

\[\mathfrak{F}[\log(|F(\omega)|^2)] = \mathfrak{F}[\log(|S(\omega)|^2)] + \mathfrak{F}[\log(|H(\omega)|^2)] \]

In practice, we were never able to get a clear separation between the source and tract signals, and could not use this method for pitch detection.

Fortunately, we found an open source pitch tracking library, dywapitchtrack \cite{2010Schmitt}.
The library was written in C, so we had to compile it into javascript with emscripten.
The library takes an array of raw audio data and returns a frequency and a confidence value. 
It also detects whether there is currently input or not. If the input is silent, it returns a null pitch value.
It was able to pitch detect a human voice very well, as well as whistling. 

\subsection{Note Separation}
Though the dywapitchtrack library neatly solved our pitch detection problem, we still do not have notes, just a stream of frequency values.
We need to determine a note's duration and pitch.
To do this the application has a buffer where we can accumulate frequency results and examine them with context.
We accumulate ten results, then break them into groups by null results.
We found that the dywapitchtrack algorithm frequently returned spurious low frequency results during periods of silence, so if any group of results would be shorter than a thirty-second note, we drop it.
Any remaining groups are assigned a note type based on their duration and passed to the layout engine. 
The note type is the largest that will fit in the duration of the result group.
The pitch assigned to the note is the median of the frequency results included. 
If there are results at the end of the buffer, we leave them to be examined in the next cycle. This way, we can avoid splitting up long notes.

\begin{figure}[h]
    \caption{An example of the spectrum and cepstrum of the human voice. \cite{ee649}}
    \includegraphics[width=0.5\textwidth]{cepstrum.pdf}
\end{figure}

\subsection{Beat Detection}
Beat detection was considerably easier than pitch detection. We wanted to be able to detect when a user clapped or snapped, and treat that like a percussion instrument.
We passed the raw audio input through a high pass filter to remove all of the melodic frequency components. 
This only left high frequency components from staccatto noises. 
Then we just registered a percussion event whenever the intensity of the resulting signal passed a certain threshold. 
We determined the cutoff frequency and power thresholed empirically.
The only way to determine how long a drum beat should be is to wait for the next drum beat, so for beat detection we set up a two element buffer. 
When a beat is detected, it sits in the buffer until the next beat is detected.
At that point we can determine the duration of the first beat, so we publish it and remove it from the buffer.
Then the second beat has to wait. 

\begin{figure}[t]
    \caption{Simple layout of web application.}
    \includegraphics[width=0.5\textwidth]{blank_page.png}
\end{figure}


\section{Output}
Our application has several modes of output. As the user is working, they are be able to listen to their composition played to them through MIDI. This gives a preview of the final result and makes it possible to edit the composition. When the user is finished with the composition, they are able to print the sheet music using standard browser printing. We had originally wanted to allow for output in MusicXML, which as a widely used music format for digital applications. This format would make this music far more extensible and compatible with other applications that use this standard. However we decided that it would be used in future iterations of this project.   For simplicity, only common time, 4 quarter notes to a measure,  were used for this musical notation, as well as not confusing sharps and flats, but resorting to all flats for any half tone. \cite{2004Baird}.

The Midi output of the application is done using the Midi.js library \cite{midi.js}. This library provides an easy interface for deciding which audio API to use, depending on the browser. The Web Audio API is a new standard that has recently been adopted by Chrome. We decided to focus on chrome for this project, as it had the best sound quality. When playing music back, the user has the option of playing all the music, or just parts of it, and is able to stop the playback at any time. When selecting a note, that note is also played briefly so the user knows the tone. The process of selecting notes and parts of music to playback will be described more in the section below.

\section{User Interface}
Our main goal for the Composer interface was to keep it as simple as possible. We wanted it to be
easy to understand without much text or unnecessary UI. However, we decided to include a small tutorial that explains the more detailed usages of the application. 

\begin{figure}[t]
    \caption{Example of Clefs. Bass and Treble clefs used for melody, bottom bar for Percussion}
    \includegraphics[width=0.5\textwidth]{clefs.png}
\end{figure}


The user interface is based primarily on the open source library Vexflow \cite{vexflow}. This library is currently in an alpha stage, and thus required some modification for our purposes. Vexflow provided the necessary tools for static layout of staves and notes, however it did not provide much in the way of animation or dynamic rendering in general. For ease of use, especially for animations, we used the SVG tools instead of Canvas, so that we could treat each note object as an element in the DOM. We created classes for each note, so that all svg elements related to that note would be linked together. This allowed for us to provide the animations detailed below. We also had to create a way to dynamically render the UI by redrawing all elements when new notes and bars were added. This provided an easy way for us to draw all notes with a few simple method calls. 

The notes are animated in with a bounce animation.As we described in our earlier proposal, animations were a key part of the user experience. We wanted the users to feel immersed in the program without unnecessary background visual "noise".  Animation has been show to increase the appeal of a user interface \cite{2008Dyer}. Check \cite{2012Joshi} for the different types of animations and transitions available with CSS3.  Once the notes are on the page, 

The main interface contains only 4 elements. For buttons, we decided to go with icons for universality. Having buttons with words would limit those people who do not understand English. We have 3 buttons, a recording button, a play button, and an info button. The play button changes to a stop button after a user starts playing the music, and the record button will change as well to indicate if the person is currently recording or not. An example of our interface can be seen in Figure 2. Originally we were going to have a button to change input types from melodic to rhythm; however we decided it would be simpler to allow recording of both at the same time depending on frequency as described previously. The info button was placed to bring up a tutorial that users may want to read to familiarize themselves with all of the controls available within the application. Figure 5 shows this tutorial

As users can generally only whistle or hum one tone, notes will be limited to single notes and not chords in this application. As a user hums or whistles, notes will appear on the screen using the bouncing animation. At any time, the user may highlight a note by double clicking on it. A highlighted note will have a red box appear around it to indicate that it has focus. The user may also at any time press the play button to begin playback. When this happens, the music will begin to playback from either the currently highlighted note, or the beginning if no note is highlighted. To unhighlight all notes, the user must double click anywhere on the application, not on another note. When the music is playing, a blue box will appear around the currently playing note. 

Highlighted notes can be edited in multiple ways. Once a note is highlighted it can be tonally changed by pressing the up and down buttons on the keyboard. This will move the note up and down a half tone respectively. The note can also be moved along the staff by pressing left and right. This will swap the current note with the note previous or after it respectively.  The user is also able to delete a note by pressing the backspace button. Figure 4 shows an example of highlighted and playing notes.

\begin{figure}[t]
    \caption{Tutorial explains all of the application controls}
    \includegraphics[width=0.5\textwidth]{tutorial.png}
\end{figure}


\subsection{Affordances}

Affordances are plenty in this application. We used icons to indicate universally what each button will do: The record button records, play button plays, and info button brings up more information about the application. Once the record button is pressed, the button pulses and indicates that the application is currently recording. As the user inputs notes, they will quickly realize that the longer one hums, the longer the note will appear on the page.

In terms of editing of notes, when a user hovers over a note, the cursor changes to a pointing hand, indicating that the note is clickable and thus editable. The tutorial explains in what ways the user can edit the notes.


\subsection{Feedback}
Feedback includes a play button that, when clicked, changes to a stop button and begins playback from the beginning or a highlighted note. Pressing this stop button will stop the music.

When a user clicks a note, the note is highlighted, indicating that the note is the current focus of the program. From there the user is able to change the tone of the note, by half tones, as well as where it is located within the music. This will be reflected automatically. When the user changes the tone of the note, the new note tone is played and the note is moved up and down vertically in the bar. When the user is moving the note left and right, these changes will be reflected immediately and the user will see the highlighted note move left and right, and the red box will move with it indicating that the note has changed positions. When a user deletes a note, all notes after the deleted note will be moved over and the highlighted box will disappear indicating that the note is gone.


\subsection{Mechanics}
The quality of the mechanics of the user interface of our app depend largely on the quality of our interpretation of the user input. Since the pitch input is fairly accurate, this provides excellent mechanics as any expert or novice will be able to use the application by doing what they already know how to do: hum or whistle. One way in which we improve upon these basic mechanics is by using note snapping and tone snapping. As humming or whistling will often be off tune a bit, we find the correct tone to match the note to, as well as the correct length depending on 120 bpm typical note lengths. 

\begin{figure}[t]
    \caption{Red boxes indicate a highlighted note. Blue boxes indicate a currently playing note}
    \includegraphics[width=0.5\textwidth]{highlighted_notes.png}
\end{figure}

\section{FUTURE WORK}
There are several areas where substantial improvement could be made to this app.
First there is a lot more work that we could do on the note detecting component. 
It currently requires you to sing very clearly and make your notes very clearly separated to pick them up well. 
It does not support rests, or keep the percurssion and melody in sync.
Finally, notes tend to get split when they occur at the end of a bar.
Fortunately we still have lots of tools at our disposal to remedy these shortcomings. 
There is a wealth of audio processing techniques that could still be incorporated into the app.

We also did not implement the MusicXML and MIDI output feature we described in our proposal. 
These features were not vital to our main focus of music composition, and would not be interesting in their implementation.
These could be easily implemented later if desired.

Another aspect of the application that could be improved is the customizability. As of now, the speed of the music is limited to 120 bps, and the time signature is in common time. Users are also not allowed to choose a key signature for the composition as a whole. We would implement this in the future by allowing users to choose from some preset key and time signatures, as well as change the timing of the music for quicker or slower playback. Another improvement with respect to customizability would be with different MIDI playback instruments. Right now we are limited to a synth drum for the percussion and a piano for the melody. We would allow users to choose from other instruments, such as a trumpet, triangle, and other common instruments.

In terms of the user interface, there is one large improvement that could be made. We had originally wanted to allow editing via drag and drop, but it proved to be more difficult to implement in the limited time we had using the alpha Vexflow Library. With more time in the future we believe that this would be possible.  Functionally, we would also like to add editing for the lengths of notes. Right now they can only be moved or tonally changed.


\nocite{*}


\bibliographystyle{plain}
\bibliography{UIProject}

\end{document}
